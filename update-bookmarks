#!/usr/bin/env python
# PYTHON_ARGCOMPLETE_OK
from functools import lru_cache
from os.path import basename, dirname, splitext
from tempfile import mkstemp
from typing import Any, Final, Sequence, TextIO, cast
import argparse
import logging
import os
import re
import sys

from bs4 import BeautifulSoup as Soup
from requests.exceptions import InvalidSchema, InvalidURL
import argcomplete
import requests
import requests.exceptions


class FailedConnection:
    REASON_CONNECTION_ERROR: Final[int] = 3
    REASON_FILE_URL: Final[int] = 4
    REASON_INVALID_SCHEMA: Final[int] = 2
    REASON_INVALID_URL: Final[int] = 1
    REASON_UNKNOWN: Final[int] = 5

    def __init__(self, response: requests.Response, reason: int, exc: Exception | None = None):
        self.response = response
        self.url = response.url
        self.reason = reason
        self.exc = exc

    def strreason(self) -> str:
        if self.reason == self.REASON_INVALID_URL:
            return f'{self.url} is not a valid URL'
        if self.reason == self.REASON_FILE_URL:
            return f'{self.url} is a file URL'
        if self.reason == self.REASON_INVALID_SCHEMA:
            return f'{self.url} has an unsupported schema'
        if self.reason == self.REASON_CONNECTION_ERROR:
            return f'Connection error: {self.exc}'
        return f'Unspecified error: {self.exc}'


@lru_cache()
def setup_logging_stdout(name: str | None = None, verbose: bool = False) -> logging.Logger:
    name = name if name else basename(sys.argv[0])
    log = logging.getLogger(name)
    log.setLevel(logging.DEBUG if verbose else logging.INFO)
    channel = logging.StreamHandler(sys.stdout)
    channel.setFormatter(logging.Formatter('%(message)s'))
    channel.setLevel(logging.DEBUG if verbose else logging.INFO)
    log.addHandler(channel)
    return log


def send(res: requests.Response | None) -> requests.Response | FailedConnection | None:
    if not res:
        return None
    try:
        res.raise_for_status()
    except InvalidURL:
        if re.match(r'^file:///', res.url):
            return FailedConnection(res, reason=FailedConnection.REASON_FILE_URL)
        return FailedConnection(res, reason=FailedConnection.REASON_INVALID_URL)
    except InvalidSchema:
        return FailedConnection(res, reason=FailedConnection.REASON_INVALID_SCHEMA)
    except requests.exceptions.ConnectionError as e:
        return FailedConnection(res, exc=e, reason=FailedConnection.REASON_CONNECTION_ERROR)
    except Exception as e:  # pylint: disable=broad-except
        return FailedConnection(res, exc=e, reason=FailedConnection.REASON_UNKNOWN)
    return res


def head(session: requests.Session, uri: str) -> requests.Response | None:
    log = setup_logging_stdout()
    try:
        return session.head(uri, timeout=5)
    except Exception as e:  # pylint: disable=broad-except
        log.error('Failed connection: %s (%s)', e, uri)
    return None


def recursive_scan(
        soup: Soup,
        session: requests.Session,
        verbose: bool = False) -> tuple[Sequence[tuple[str, str]], Sequence[tuple[str, Any]]]:
    log = setup_logging_stdout(verbose=verbose)
    ret = []
    errors = []
    urls: list[tuple[str, str]] = []
    assert soup.body is not None
    for anchor in soup.body.select('a'):
        title = ' '.join(i.get_text() for i in anchor.contents)
        href = anchor['href']
        assert isinstance(href, str)
        urls.append((title, href))
    index = 0
    for req, url_data in zip(map(send, (head(session, i[1]) for i in urls)), urls):
        if not req:
            log.error('Failed connection: "%s" @ "%s", reason: other', url_data[0], url_data[1])
            index += 1
            continue
        if isinstance(req, FailedConnection):
            log.error('Failed connection: "%s" @ "%s", reason: %s', url_data[0], url_data[1],
                      req.strreason())
            index += 1
            continue
        if req.status_code != 200:
            if req.status_code in (301, 302):
                log.info('%d: "%s" @ "%s" -> now "%s"', req.status_code, url_data[0], url_data[1],
                         req.headers['location'])
                ret.append((url_data[1], req.headers['location']))
            elif req.status_code not in [401, 403]:
                log.error('%d: "%s" @ "%s"', req.status_code, url_data[0], url_data[1])
                errors.append(url_data)
        else:
            log.debug('200: "%s"', url_data[1])
        index += 1
    return ret, errors


class Namespace(argparse.Namespace):
    file: Sequence[TextIO]
    limit: int
    output: str | None
    quiet: bool


def main() -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument('file',
                        metavar='IN_FILE',
                        type=argparse.FileType('r'),
                        nargs=1,
                        help='Bookmark HTML file (usually exported from browser)')
    parser.add_argument('-o',
                        '--output',
                        metavar='OUTFILE',
                        nargs=1,
                        help=('File to output to (will be determined automatically if not '
                              'specified)'))
    parser.add_argument('-q', '--quiet', action='store_true', help='Quiet mode')
    parser.add_argument('-l', '--limit', type=int, help='Number of concurrent requests', default=2)
    argcomplete.autocomplete(parser)
    args = cast(Namespace, parser.parse_args())
    f = args.file[0]
    log = setup_logging_stdout(verbose=not args.quiet)
    session = requests.Session()
    session.headers.update({
        'user-agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.29 '
                       'Safari/537.36'),
        'upgrade-insecure-requests': '1',
        'referer': 'https://www.google.com/',
        'pragma': 'no-cache',
        'dnt': '1',
        'cache-control': 'no-cache',
    })
    contents = f.read()
    replacements, errors = recursive_scan(Soup(contents, 'lxml'), requests.Session())
    for find, repl in replacements:
        contents = contents.replace(find, repl)
    output_fd = None
    if not args.output:
        output_dir = dirname(f.name)
        output_file = 'new-' + splitext(basename(f.name))[0]
        output_fd, output_file = mkstemp(prefix=output_file, dir=output_dir, suffix='.html')
        log.info('Writing output to "%s"', output_file)
    if output_fd:
        os.write(output_fd, contents.encode('utf-8'))
        os.close(output_fd)
    else:
        assert args.output
        with open(args.output, 'w') as f:
            f.write(contents)
    log.info('Updated %d URLs', len(replacements))
    log.info('%d URLs had unrecoverable errors', len(errors))
    return 0


if __name__ == '__main__':
    sys.exit(main())
